{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"170104129_exp_03_02.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HT9i40sMM5lt","executionInfo":{"status":"ok","timestamp":1630829864431,"user_tz":-360,"elapsed":4665,"user":{"displayName":"Ahmed Rad Sakib","photoUrl":"","userId":"13665726696432201211"}}},"source":["import os\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader  \n","from skimage import io"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvahDt2be8xz","executionInfo":{"status":"ok","timestamp":1630829866459,"user_tz":-360,"elapsed":3,"user":{"displayName":"Ahmed Rad Sakib","photoUrl":"","userId":"13665726696432201211"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjT53blqLYw3","executionInfo":{"status":"ok","timestamp":1630829857923,"user_tz":-360,"elapsed":23950,"user":{"displayName":"Ahmed Rad Sakib","photoUrl":"","userId":"13665726696432201211"}},"outputId":"40570367-b165-4843-9e85-278c46067a09"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"my-Lx-VJI5zT","executionInfo":{"status":"ok","timestamp":1630829874483,"user_tz":-360,"elapsed":5093,"user":{"displayName":"Ahmed Rad Sakib","photoUrl":"","userId":"13665726696432201211"}},"outputId":"05f42f12-51fb-428d-bdf0-1f148a33bc6b"},"source":["!unzip \"/content/drive/MyDrive/Colab Notebooks/assignment2/Dataset_2.zip\""],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/Colab Notebooks/assignment2/Dataset_2.zip\n","   creating: Dataset_2/\n","  inflating: Dataset_2/t10k-images-idx3-ubyte  \n","  inflating: Dataset_2/t10k-labels-idx1-ubyte  \n","  inflating: Dataset_2/train-images-idx3-ubyte  \n","  inflating: Dataset_2/train-labels-idx1-ubyte  \n","  inflating: Dataset_2/verification_test.csv  \n","  inflating: Dataset_2/verification_train.csv  \n"]}]},{"cell_type":"code","metadata":{"id":"WpoWmdLjJLML","executionInfo":{"status":"ok","timestamp":1630829880554,"user_tz":-360,"elapsed":3803,"user":{"displayName":"Ahmed Rad Sakib","photoUrl":"","userId":"13665726696432201211"}}},"source":["train_csv  = pd.read_csv('/content/Dataset_2/verification_train.csv')\n","test_csv = pd.read_csv('/content/Dataset_2/verification_test.csv')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtmU_CAKJlVN","executionInfo":{"status":"ok","timestamp":1630829883340,"user_tz":-360,"elapsed":782,"user":{"displayName":"Ahmed Rad Sakib","photoUrl":"","userId":"13665726696432201211"}}},"source":["import numpy as np\n","class second_Dataset(Dataset):\n","    \"\"\"User defined class to build a datset using Pytorch class Dataset.\"\"\"\n","    \n","    def __init__(self, data, transform = None):\n","        \"\"\"Method to initilaize variables.\"\"\" \n","        self.data_MNIST = list(data.values)\n","        self.transform = transform\n","        \n","        label = []\n","        image = []\n","        \n","        for i in self.data_MNIST:\n","             # first column is of labels.\n","            label.append(i[0])\n","            image.append(i[1:])\n","        self.labels = np.asarray(label)\n","        self.images = np.asarray(image).reshape(-1, 28, 28, 1).astype('float32')\n","\n","    def __getitem__(self, index):\n","        label = self.labels[index]\n","        image = self.images[index]\n","        \n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        return image, label\n","\n","    def __len__(self):\n","        return len(self.images)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1jEAEZCKQIU","executionInfo":{"status":"ok","timestamp":1630829887053,"user_tz":-360,"elapsed":487,"user":{"displayName":"Ahmed Rad Sakib","photoUrl":"","userId":"13665726696432201211"}}},"source":["train_set = second_Dataset(train_csv, transform=transforms.Compose([transforms.ToTensor()]))\n","test_set = second_Dataset(test_csv, transform=transforms.Compose([transforms.ToTensor()]))\n","\n","train_loader = DataLoader(train_set, batch_size=100)\n","test_loader = DataLoader(train_set, batch_size=100)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Za7FBndHL3HN","executionInfo":{"status":"ok","timestamp":1630829889316,"user_tz":-360,"elapsed":3,"user":{"displayName":"Ahmed Rad Sakib","photoUrl":"","userId":"13665726696432201211"}},"outputId":"4039ce86-dd50-4db3-b9c4-e1e4c4238e8f"},"source":["print(len(train_set))\n","print(len(test_set))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["60000\n","10000\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdJtSw82MSSB","outputId":"a722834a-aeef-476a-975a-4cc4e8a8151c"},"source":["# Hyperparameters\n","\n","batch_size = 200\n","num_iters = 20000\n","input_dim = 28*28\n","num_hidden = 300\n","output_dim = 10\n","\n","learning_rate = 0.01 \n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","\n","\n","'''\n","MAKING DATASET ITERABLE\n","'''\n","num_epochs = num_iters / (len(train_set) / batch_size)\n","num_epochs = int(num_epochs)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_set, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)   # It's better to shuffle the whole training dataset! \n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_set, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)\n","\n","\n","\n","\n","\n","class DeepNeuralNetworkModel(nn.Module):\n","    def __init__(self, input_size, num_classes, num_hidden):\n","        super().__init__()\n","       \n","        self.linear_1 = nn.Linear(input_size, num_hidden)\n","        self.relu_1 = nn.ReLU()\n"," \n","        self.linear_2 = nn.Linear(num_hidden, num_hidden)\n","        self.relu_2 = nn.ReLU()\n"," \n","        self.linear_3 = nn.Linear(num_hidden, num_hidden)\n","        self.relu_3 = nn.ReLU()\n"," \n","        self.linear_4 = nn.Linear(num_hidden, num_hidden)\n","        self.relu_4 = nn.Softmax(dim=0)\n"," \n","        self.linear_5= nn.Linear(num_hidden, num_hidden)\n","        self.relu_5= nn.ReLU()\n"," \n","        self.linear_6 = nn.Linear(num_hidden, num_hidden)\n","        self.relu_6 = nn.ReLU()\n"," \n","        self.linear_out = nn.Linear(num_hidden, num_classes)\n"," \n","    def forward(self, x):\n","        out  = self.linear_1(x)\n","        out = self.relu_1(out)\n","        \n","        out  = self.linear_2(out)\n","        out = self.relu_2(out)\n"," \n","        out  = self.linear_3(out)\n","        out = self.relu_3(out)\n"," \n","        out  = self.linear_4(out)\n","        out = self.relu_4(out)\n"," \n","        out  = self.linear_5(out)\n","        out = self.relu_5(out)\n"," \n","        out  = self.linear_6(out)\n","        out = self.relu_6(out)\n","        \n","        probas  = self.linear_out(out)\n","        return probas\n","\n","# INSTANTIATE MODEL CLASS\n","\n","model = DeepNeuralNetworkModel(input_size = input_dim,\n","                               num_classes = output_dim,\n","                               num_hidden = num_hidden)\n","# To enable GPU\n","model.to(device)\n","\n","# INSTANTIATE LOSS & OPTIMIZER CLASS\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","iteration_loss= []\n","iter = 0\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","\n","        images = images.view(-1, 28*28).to(device)\n","        labels = labels.to(device)\n","\n","        # Clear gradients w.r.t. parameters\n","        optimizer.zero_grad()\n","\n","        # Forward pass to get output/logits\n","        outputs = model(images) \n","\n","        # Calculate Loss: softmax --> cross entropy loss\n","        loss = criterion(outputs, labels)\n","\n","        # Getting gradients w.r.t. parameters\n","        loss.backward()\n","\n","        # Updating parameters\n","        optimizer.step()\n","\n","        iter += 1\n","\n","        if iter % 500 == 0:\n","            # Calculate Accuracy         \n","            correct = 0\n","            total = 0\n","            # Iterate through test dataset\n","            for images, labels in test_loader:\n","               \n","                images = images.view(-1, 28*28).to(device)\n","\n","                # Forward pass only to get logits/output\n","                outputs = model(images)\n","\n","                # Get predictions from the maximum value\n","                _, predicted = torch.max(outputs, 1)\n","\n","                # Total number of labels\n","                total += labels.size(0)\n","\n","\n","                # Total correct predictions\n","                if torch.cuda.is_available():\n","                    correct += (predicted.cpu() == labels.cpu()).sum() \n","                else:\n","                    correct += (predicted == labels).sum()\n","\n","            accuracy = 100 * correct.item() / total\n","\n","            # Print Loss\n","            iteration_loss.append(loss.item())\n","            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 500. Loss: 2.3009088039398193. Accuracy: 14.93\n","Iteration: 1000. Loss: 2.2990190982818604. Accuracy: 19.68\n","Iteration: 1500. Loss: 2.2955846786499023. Accuracy: 34.47\n","Iteration: 2000. Loss: 2.293377161026001. Accuracy: 45.5\n","Iteration: 2500. Loss: 2.2891321182250977. Accuracy: 56.31\n","Iteration: 3000. Loss: 2.2843217849731445. Accuracy: 50.9\n","Iteration: 3500. Loss: 2.278231620788574. Accuracy: 50.02\n","Iteration: 4000. Loss: 2.271796941757202. Accuracy: 48.8\n","Iteration: 4500. Loss: 2.2588958740234375. Accuracy: 53.2\n","Iteration: 5000. Loss: 2.242007255554199. Accuracy: 52.18\n","Iteration: 5500. Loss: 2.2163949012756348. Accuracy: 48.79\n","Iteration: 6000. Loss: 2.170585870742798. Accuracy: 52.39\n","Iteration: 6500. Loss: 2.1021344661712646. Accuracy: 51.26\n","Iteration: 7000. Loss: 1.9991064071655273. Accuracy: 57.01\n","Iteration: 7500. Loss: 1.8869060277938843. Accuracy: 58.14\n","Iteration: 8000. Loss: 1.6901516914367676. Accuracy: 60.13\n","Iteration: 8500. Loss: 1.5001189708709717. Accuracy: 66.17\n","Iteration: 9000. Loss: 1.3504014015197754. Accuracy: 68.99\n","Iteration: 9500. Loss: 1.1744834184646606. Accuracy: 72.64\n","Iteration: 10000. Loss: 1.0671559572219849. Accuracy: 75.62\n","Iteration: 10500. Loss: 1.0188099145889282. Accuracy: 75.45\n","Iteration: 11000. Loss: 0.9407618641853333. Accuracy: 79.24\n","Iteration: 11500. Loss: 0.8375665545463562. Accuracy: 79.63\n","Iteration: 12000. Loss: 0.6929500699043274. Accuracy: 81.22\n","Iteration: 12500. Loss: 0.6250252723693848. Accuracy: 81.6\n"]}]},{"cell_type":"code","metadata":{"id":"Egeoid1fNCBc"},"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","\n","print (iteration_loss)\n","plt.plot(iteration_loss)\n","plt.ylabel('Loss')\n","plt.xlabel('Iteration (in every 500)')\n","plt.show()"],"execution_count":null,"outputs":[]}]}